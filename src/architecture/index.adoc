= Cluster Logging Architecture
:toc: left

== Architecture Summary
=== Log categories

We define 3 _logging categories_

[horizontal]
Application:: Container logs from non-infrastrure containers.
Infrastructure:: Node logs and container logs from `kube-*` and `openshift-*` namespaces.
Audit:: Node logs from `/var/log/audit`, security sensitive.


=== Components

The logging system breaks down into 4 logical components:

[horizontal]
collector:: Read container log data from each node.
forwarder:: Forward log data to configured outputs.
store:: Store log data for analys This is the default output for the _forwarder_.
exploration:: UI tools (GUI and command line) to search, query and view stored logs

=== Operators and Custom Resources


.Key to diagrams
image::legend.svg[]

.Operators and APIs
image::overview.svg[]

The *cluster logging operator (CLO)* implements the following custom resources:

ClusterLogging (CL)::
  Deploys the _collector_ and _forwarder_ which currently are both implemented by a _daemonset_ running _Fluentd_ on each node.
ClusterLogForwarder (CLF)::
  Generate Fluentd configuration to forward logs per user configuration.

The *elasticsearch logging operator (ELO)* implements the following custom resources:

ElasticSearch::
  Configure and deploy an Elasticsearch instance as the default log store.
Kibana::
  Configure and deploy Kibana instance to search, query and view logs.

=== Runtime behavior

.Collection and forwarding
image::node.svg[]

The _container runtime interface_ (CRI-O) on each node writes container logs to files.
The file names include the container's UID, namespace, name and other data.
We also collect per-node logs from the Linux _journald_.

The CLO deploys a Fluentd daemon on each node which acts both as a _collector_ (reading log files) and as a _forwarder_ (sending log data to configured outputs)

=== Log Entries ===

The term _log_ is overloaded so we'll use these terms to clarify:

Log:: A stream of text containing a sequence of _log entries_.

Log Entry::  Usually a single line of text in a log, but see <<multi-line-entries>>

Container Log:: Produced by CRI-O, combining stdout/stderr output from process running in a single container.

Node Log:: Produced by journald or other per-node agent from non-containerized processes on the node.

Structured Log::  A log where each entry is a JSON object (map), written as a single line of text.

Kubernetes does not enforce a uniform format for logs.
Anything that a containerized process writes to `stdout` or `stderr` is considered a log.
This "lowest common denominator" approach allows pre-existing applications to run on the cluster.

Traditional log formats write entries as ordered fields, but the order, field separator, format and meaning of fields varies.

_Structured logs_ write log entries as JSON objects on a single line.
However names, types, and meaning of fields in the JSON object varies between applications.

The https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging[Kubernetes Structured Logging proposal] will standardize the log format for _some_ k8s components, but there will still be diverse log formats from non-k8s applications running on the cluster.

==== Metadata, Envelopes and Forwarding

_Metadata_ is additional data about a log entry (original host, container-id, namespace etc.) that we add as part of forwarding the logs. We use these terms for clarity:

[horizontal]
Message:: The original, unmodifed log entry.

Envelope:: Include metadata fields and a `message` field with the original _message_

We usually use JSON notation for the envelope since it's the most widespread convention.

However, we do and will implement other output formats formats; for example a syslog message with its `MSG` and `STRUCTURED-DATA` sections is an different way to encode the equivalent envelope data.

Depending on the output type, we may forward entries as __message_ only, full _envelope_, or the users choice.

CAUTION: Not all of the documented model is in active use. Review is needed.
The `labels` field is "flattened" before forwarding to Elasticsearch.

==== Multi-line Entries

Log entries are usually a single line of text, but they can consist of more than one line for several reasons:

CRI-O::
CRI-O reads chunks of text from applications, not single lines. If a line gets split between chunks, CRI-O writes each part as a separate line in the log file with a "partial" flag so they can be correctly re-assembled.

Stack traces::
Programs in languages like Java, Ruby or Python often dump multi-line stack traces into the log. The entire stack trace needs to be kept together when forwarded to be useful.

JSON Objects::
A JSON object _can_ be written on multiple lines, although structured logging libraries typically don't do this.

